{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## environment class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grid:\n",
    "    def __init__(self, rows, cols, start):\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.i = start[0]\n",
    "        self.j = start[1]\n",
    "\n",
    "        \n",
    "    def set(self, rewards, actions):\n",
    "        # rewards should be a dict of: (i, j): r (row, col): reward\n",
    "        # actions should be a dict of: (i, j): A (row, col): list of possible actions\n",
    "        self.rewards = rewards\n",
    "        self.actions = actions\n",
    "\n",
    "        \n",
    "    def set_state(self, s):\n",
    "        self.i = s[0]\n",
    "        self.j = s[1]\n",
    "\n",
    "        \n",
    "    def current_state(self):\n",
    "        return (self.i, self.j)\n",
    "\n",
    "    \n",
    "    def is_terminal(self, s):\n",
    "        return s not in self.actions\n",
    "\n",
    "    \n",
    "    def move(self, action):\n",
    "        # check if legal move first\n",
    "        if action in self.actions[(self.i, self.j)]:\n",
    "            if action == 'U':\n",
    "                self.i -= 1\n",
    "            elif action == 'D':\n",
    "                self.i += 1\n",
    "            elif action == 'R':\n",
    "                self.j += 1\n",
    "            elif action == 'L':\n",
    "                self.j -= 1\n",
    "        # return a reward (if any)\n",
    "        return self.rewards.get((self.i, self.j), 0)\n",
    "\n",
    "    \n",
    "    def undo_move(self, action):\n",
    "        # these are the opposite of what U/D/L/R should normally do\n",
    "        if action == 'U':\n",
    "            self.i += 1\n",
    "        elif action == 'D':\n",
    "            self.i -= 1\n",
    "        elif action == 'R':\n",
    "            self.j -= 1\n",
    "        elif action == 'L':\n",
    "            self.j += 1\n",
    "        # raise an exception if we arrive somewhere we shouldn't be\n",
    "        # should never happen\n",
    "        assert(self.current_state() in self.all_states())\n",
    "\n",
    "    \n",
    "    def game_over(self):\n",
    "        # returns true if game is over, else false\n",
    "        # true if we are in a state where no actions are possible\n",
    "        return (self.i, self.j) not in self.actions\n",
    "\n",
    "    \n",
    "    def all_states(self):\n",
    "        # possibly buggy but simple way to get all states\n",
    "        # either a position that has possible next actions\n",
    "        # or a position that yields a reward\n",
    "        return set(self.actions.keys()) | set(self.rewards.keys())\n",
    "\n",
    "\n",
    "def standard_grid():\n",
    "    # define a grid that describes the reward for arriving at each state\n",
    "    # and possible actions at each state\n",
    "    # the grid looks like this\n",
    "    # x means you can't go there\n",
    "    # s means start position\n",
    "    # number means reward at that state\n",
    "    # .  .  .  1\n",
    "    # .  x  . -1\n",
    "    # s  .  .  .\n",
    "    g = Grid(3, 4, (2, 0))\n",
    "    rewards = {(0, 3): 1, (1, 3): -1}\n",
    "    actions = {\n",
    "        (0, 0): ('D', 'R'),\n",
    "        (0, 1): ('L', 'R'),\n",
    "        (0, 2): ('L', 'D', 'R'),\n",
    "        (1, 0): ('U', 'D'),\n",
    "        (1, 2): ('U', 'D', 'R'),\n",
    "        (2, 0): ('U', 'R'),\n",
    "        (2, 1): ('L', 'R'),\n",
    "        (2, 2): ('L', 'R', 'U'),\n",
    "        (2, 3): ('L', 'U'),\n",
    "    }\n",
    "    g.set(rewards, actions)\n",
    "    return g\n",
    "\n",
    "\n",
    "def negative_grid(step_cost=-0.1):\n",
    "    # in this game we want to try to minimize the number of moves\n",
    "    # so we will penalize every move\n",
    "    g = standard_grid()\n",
    "    g.rewards.update({\n",
    "        (0, 0): step_cost,\n",
    "        (0, 1): step_cost,\n",
    "        (0, 2): step_cost,\n",
    "        (1, 0): step_cost,\n",
    "        (1, 2): step_cost,\n",
    "        (2, 0): step_cost,\n",
    "        (2, 1): step_cost,\n",
    "        (2, 2): step_cost,\n",
    "        (2, 3): step_cost,\n",
    "    })\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_value(V, g):\n",
    "    for i in range(g.rows):\n",
    "        print('----------------------------')\n",
    "        for j in range(g.cols):\n",
    "            v = V.get((i,j), 0)\n",
    "            if v>=0:\n",
    "                print('| %.2f '%v, end='')\n",
    "            else:\n",
    "                print('|%.2f '%v, end='') # negative sign takes up an extra space\n",
    "        print('|')\n",
    "    print('----------------------------')\n",
    "    \n",
    "\n",
    "def print_policy(P, g):\n",
    "    for i in range(g.rows):\n",
    "        print('-----------------')\n",
    "        for j in range(g.cols):\n",
    "            a = P.get((i,j), ' ')\n",
    "            print('| %s '%a, end='')\n",
    "        print('|')\n",
    "    print('-----------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo (MC) method with Approximation\n",
    "here we generate a feature vector assuming all the states are categorical data<br>\n",
    "so for each categorical data we genearate a feature vector like in neural networks where each feature will belong to Real number<br>\n",
    "each feature is an array- [state[0], state[1], state[0]\\*state[1], 1] <br>\n",
    "here we will replace V(s) with linear model as following- <br>\n",
    "V(s) = V(s) + alpha\\*(G(s) - V(s)) where G(s) = r + gamma\\*G(s')<br>\n",
    "theta = theta + alpha\\*(G(s) - V(s, theta))\\*feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## windy environment or non deterministic action\n",
    "notice that we are not optimizing policy here just general a new set of values which will be a little more negative compared to previous approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_action(a):\n",
    "    p = np.random.random()\n",
    "    if p<0.5:\n",
    "        return a\n",
    "    else:\n",
    "        return np.random.choice([action for action in all_possible_actions if action!=a])\n",
    "\n",
    "def play_game(grid, policy):\n",
    "    # returns a list of states and corresponding returns\n",
    "    # reset game to start at a random position\n",
    "    # we need to do this, because given our current deterministic policy\n",
    "    # we would never end up at certain states, but we still want to measure their value\n",
    "    \n",
    "    start_states = list(grid.actions.keys())\n",
    "    start_idx = np.random.choice(len(start_states))\n",
    "    grid.set_state(start_states[start_idx])\n",
    "\n",
    "    s = grid.current_state()\n",
    "    states_rewards = [(s, 0)]  # list of tuples (state, reward)\n",
    "    while not grid.game_over():\n",
    "        a = policy[s]\n",
    "        a = random_action(a)\n",
    "        r = grid.move(a)\n",
    "        s = grid.current_state() \n",
    "        states_rewards.append((s, r))\n",
    "    # calculate the returns by working backwards from the terminal state\n",
    "    G=0\n",
    "    states_returns = []\n",
    "    first=True\n",
    "    for s, r in reversed(states_rewards):\n",
    "        # the value of the terminal state is 0 by definition\n",
    "        # we should ignore the first state we encounter\n",
    "        # and ignore the last G, which is meaningless since it doesn't correspond to any move\n",
    "        if first:\n",
    "            first = False\n",
    "        else:\n",
    "            states_returns.append((s, G))\n",
    "        G = r + gamma*G\n",
    "    states_returns.reverse() # we want it to be in order of state visited\n",
    "    return states_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "----------------------------\n",
      "| 0.00 | 0.00 | 0.00 | 1.00 |\n",
      "----------------------------\n",
      "| 0.00 | 0.00 | 0.00 |-1.00 |\n",
      "----------------------------\n",
      "| 0.00 | 0.00 | 0.00 | 0.00 |\n",
      "----------------------------\n",
      "values:\n",
      "----------------------------\n",
      "| 0.40 | 0.54 | 0.68 | 0.00 |\n",
      "----------------------------\n",
      "| 0.34 | 0.00 | 0.32 | 0.00 |\n",
      "----------------------------\n",
      "| 0.27 | 0.12 |-0.04 |-0.19 |\n",
      "----------------------------\n",
      "policy:\n",
      "-----------------\n",
      "| R | R | R |   |\n",
      "-----------------\n",
      "| U |   | U |   |\n",
      "-----------------\n",
      "| U | L | U | L |\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.9 # discount factor\n",
    "\n",
    "# learning rate\n",
    "lr = 0.001\n",
    "\n",
    "# set different actions possible for a particular state\n",
    "all_possible_actions = ['U', 'D', 'R', 'L']\n",
    "\n",
    "# use the standard grid again (0 for every step) so that we can compare\n",
    "# to iterative policy evaluation\n",
    "grid = standard_grid()\n",
    "\n",
    "# print rewards\n",
    "print(\"rewards:\")\n",
    "print_value(grid.rewards, grid)\n",
    "\n",
    "# state -> action\n",
    "policy = {\n",
    "    (2, 0): 'U',\n",
    "    (1, 0): 'U',\n",
    "    (0, 0): 'R',\n",
    "    (0, 1): 'R',\n",
    "    (0, 2): 'R',\n",
    "    (1, 2): 'U',\n",
    "    (2, 1): 'L',\n",
    "    (2, 2): 'U',\n",
    "    (2, 3): 'L'\n",
    "}\n",
    "\n",
    "\n",
    "# initialize theta\n",
    "# our model is V_hat = theta.dot(x)\n",
    "# where x = [row, col, row*col, 1] - 1 for bias term\n",
    "theta = np.random.randn(4)/2   # theta with a array of 4 values\n",
    "def s2x(s):\n",
    "    return np.array([s[0] - 1, s[1] - 1.5, s[0]*s[1], 1])\n",
    "\n",
    "# play game for n episodes\n",
    "i = 1.0\n",
    "for t in range(20000):\n",
    "    if t % 100 == 0:\n",
    "        i += 0.01\n",
    "    alpha = lr/i\n",
    "    \n",
    "    # generate an episode using pi\n",
    "    states_returns = play_game(grid, policy)\n",
    "    \n",
    "    seen_states = set()\n",
    "    for s, G in states_returns:\n",
    "        # check if we have already seen s\n",
    "        # called \"first-visit\" MC policy evaluation\n",
    "        # in this if a state repeats again in a particular episode then we consider the value of only first visit\n",
    "        if s not in seen_states:\n",
    "            x = s2x(s)\n",
    "            V_hat = theta.dot(x)\n",
    "            # theta = theta + alpha*(G(s) - V(s, theta))*x\n",
    "            theta += alpha*(G - V_hat)*x    # theta- a vector/array of 4 values\n",
    "            seen_states.add(s)\n",
    "            \n",
    "# obtain predicted values\n",
    "states = grid.all_states()\n",
    "V = {}\n",
    "for s in states:\n",
    "    if s in grid.actions:\n",
    "        # this is like in the last layer of neural network where weights and bias are multiplied to a new\n",
    "        # feature to generate a final value for a regression task\n",
    "        V[s] = theta.dot(s2x(s))\n",
    "    else:\n",
    "        # terminal state or state we can't otherwise get to\n",
    "        V[s] = 0\n",
    "        \n",
    "print(\"values:\")\n",
    "print_value(V, grid)\n",
    "    \n",
    "print('policy:')\n",
    "print_policy(policy, grid)\n",
    "    \n",
    "# state -> action\n",
    "# found by policy_iteration_random on standard_grid\n",
    "# MC method won't get exactly this, but should be close\n",
    "# values:\n",
    "# ---------------------------\n",
    "#  0.43|  0.56|  0.72|  0.00|\n",
    "# ---------------------------\n",
    "#  0.33|  0.00|  0.21|  0.00|\n",
    "# ---------------------------\n",
    "#  0.25|  0.18|  0.11| -0.17|\n",
    "# policy:\n",
    "# ---------------------------\n",
    "#   R  |   R  |   R  |      |\n",
    "# ---------------------------\n",
    "#   U  |      |   U  |      |\n",
    "# ---------------------------\n",
    "#   U  |   L  |   U  |   L  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
