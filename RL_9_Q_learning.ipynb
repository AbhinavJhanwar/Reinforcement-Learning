{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## environment class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grid:\n",
    "    def __init__(self, rows, cols, start):\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.i = start[0]\n",
    "        self.j = start[1]\n",
    "\n",
    "        \n",
    "    def set(self, rewards, actions):\n",
    "        # rewards should be a dict of: (i, j): r (row, col): reward\n",
    "        # actions should be a dict of: (i, j): A (row, col): list of possible actions\n",
    "        self.rewards = rewards\n",
    "        self.actions = actions\n",
    "\n",
    "        \n",
    "    def set_state(self, s):\n",
    "        self.i = s[0]\n",
    "        self.j = s[1]\n",
    "\n",
    "        \n",
    "    def current_state(self):\n",
    "        return (self.i, self.j)\n",
    "\n",
    "    \n",
    "    def is_terminal(self, s):\n",
    "        return s not in self.actions\n",
    "\n",
    "    \n",
    "    def move(self, action):\n",
    "        # check if legal move first\n",
    "        if action in self.actions[(self.i, self.j)]:\n",
    "            if action == 'U':\n",
    "                self.i -= 1\n",
    "            elif action == 'D':\n",
    "                self.i += 1\n",
    "            elif action == 'R':\n",
    "                self.j += 1\n",
    "            elif action == 'L':\n",
    "                self.j -= 1\n",
    "        # return a reward (if any)\n",
    "        return self.rewards.get((self.i, self.j), 0)\n",
    "\n",
    "    \n",
    "    def undo_move(self, action):\n",
    "        # these are the opposite of what U/D/L/R should normally do\n",
    "        if action == 'U':\n",
    "            self.i += 1\n",
    "        elif action == 'D':\n",
    "            self.i -= 1\n",
    "        elif action == 'R':\n",
    "            self.j -= 1\n",
    "        elif action == 'L':\n",
    "            self.j += 1\n",
    "        # raise an exception if we arrive somewhere we shouldn't be\n",
    "        # should never happen\n",
    "        assert(self.current_state() in self.all_states())\n",
    "\n",
    "    \n",
    "    def game_over(self):\n",
    "        # returns true if game is over, else false\n",
    "        # true if we are in a state where no actions are possible\n",
    "        return (self.i, self.j) not in self.actions\n",
    "\n",
    "    \n",
    "    def all_states(self):\n",
    "        # possibly buggy but simple way to get all states\n",
    "        # either a position that has possible next actions\n",
    "        # or a position that yields a reward\n",
    "        return set(self.actions.keys()) | set(self.rewards.keys())\n",
    "\n",
    "\n",
    "def standard_grid():\n",
    "    # define a grid that describes the reward for arriving at each state\n",
    "    # and possible actions at each state\n",
    "    # the grid looks like this\n",
    "    # x means you can't go there\n",
    "    # s means start position\n",
    "    # number means reward at that state\n",
    "    # .  .  .  1\n",
    "    # .  x  . -1\n",
    "    # s  .  .  .\n",
    "    g = Grid(3, 4, (2, 0))\n",
    "    rewards = {(0, 3): 1, (1, 3): -1}\n",
    "    actions = {\n",
    "        (0, 0): ('D', 'R'),\n",
    "        (0, 1): ('L', 'R'),\n",
    "        (0, 2): ('L', 'D', 'R'),\n",
    "        (1, 0): ('U', 'D'),\n",
    "        (1, 2): ('U', 'D', 'R'),\n",
    "        (2, 0): ('U', 'R'),\n",
    "        (2, 1): ('L', 'R'),\n",
    "        (2, 2): ('L', 'R', 'U'),\n",
    "        (2, 3): ('L', 'U'),\n",
    "    }\n",
    "    g.set(rewards, actions)\n",
    "    return g\n",
    "\n",
    "\n",
    "def negative_grid(step_cost=-0.1):\n",
    "    # in this game we want to try to minimize the number of moves\n",
    "    # so we will penalize every move\n",
    "    g = standard_grid()\n",
    "    g.rewards.update({\n",
    "        (0, 0): step_cost,\n",
    "        (0, 1): step_cost,\n",
    "        (0, 2): step_cost,\n",
    "        (1, 0): step_cost,\n",
    "        (1, 2): step_cost,\n",
    "        (2, 0): step_cost,\n",
    "        (2, 1): step_cost,\n",
    "        (2, 2): step_cost,\n",
    "        (2, 3): step_cost,\n",
    "    })\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_value(V, g):\n",
    "    for i in range(g.rows):\n",
    "        print('----------------------------')\n",
    "        for j in range(g.cols):\n",
    "            v = V.get((i,j), 0)\n",
    "            if v>=0:\n",
    "                print('| %.2f '%v, end='')\n",
    "            else:\n",
    "                print('|%.2f '%v, end='') # negative sign takes up an extra space\n",
    "        print('|')\n",
    "    print('----------------------------')\n",
    "    \n",
    "\n",
    "def print_policy(P, g):\n",
    "    for i in range(g.rows):\n",
    "        print('-----------------')\n",
    "        for j in range(g.cols):\n",
    "            a = P.get((i,j), ' ')\n",
    "            print('| %s '%a, end='')\n",
    "        print('|')\n",
    "    print('-----------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Difference or TD0 method\n",
    "advantage of this method is that it combines both dynamic and monte carlo programmin\n",
    "i.e. it overcomes the limitation of mc where we have to wait for an episode to finish to calculate Q value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizing policy using Q learning\n",
    "The goal is to maximize the Q-value. Q-learning belongs to the off-policy category i.e. the next action is chosen to maximize the next stateâ€™s Q-value instead of following the epsilon greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_action(a, eps=0.1):\n",
    "    # choose given a with probability 1 - eps + eps/4\n",
    "    # choose some other a' != a with probability eps/4\n",
    "    p = np.random.random()\n",
    "    \n",
    "    # if p < (1 - eps + eps/len(ALL_POSSIBLE_ACTIONS)):\n",
    "    #   return a\n",
    "    # else:\n",
    "    #   tmp = list(ALL_POSSIBLE_ACTIONS)\n",
    "    #   tmp.remove(a)\n",
    "    #   return np.random.choice(tmp)\n",
    "    #\n",
    "    # this is equivalent to the above\n",
    "    if p<(1-eps):\n",
    "        return a\n",
    "    else:\n",
    "        return np.random.choice(all_possible_actions)\n",
    "    \n",
    "    \n",
    "def max_dict(d):\n",
    "    # returns the argmax (key) and max (value) from a dictionary\n",
    "    max_key = None\n",
    "    max_value = float('-inf')\n",
    "    for key, value in d.items():\n",
    "        if value>max_value:\n",
    "            max_key = key\n",
    "            max_value = value\n",
    "    return max_key, max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "----------------------------\n",
      "|-0.10 |-0.10 |-0.10 | 1.00 |\n",
      "----------------------------\n",
      "|-0.10 | 0.00 |-0.10 |-1.00 |\n",
      "----------------------------\n",
      "|-0.10 |-0.10 |-0.10 |-0.10 |\n",
      "----------------------------\n",
      "policy:\n",
      "-----------------\n",
      "| R | R | R |   |\n",
      "-----------------\n",
      "| U |   | U |   |\n",
      "-----------------\n",
      "| R | R | U | L |\n",
      "-----------------\n",
      "values:\n",
      "----------------------------\n",
      "| 0.61 | 0.80 | 1.00 | 0.00 |\n",
      "----------------------------\n",
      "| 0.40 | 0.00 | 0.80 | 0.00 |\n",
      "----------------------------\n",
      "| 0.31 | 0.46 | 0.62 | 0.45 |\n",
      "----------------------------\n",
      "update counts:\n",
      "----------------------------\n",
      "| 0.01 | 0.01 | 0.19 | 0.00 |\n",
      "----------------------------\n",
      "| 0.01 | 0.00 | 0.19 | 0.00 |\n",
      "----------------------------\n",
      "| 0.20 | 0.20 | 0.19 | 0.01 |\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "# let's see how V(s) changes as we get further away from the reward\n",
    "gamma = 0.9 # discount factor\n",
    "\n",
    "# set different actions possible for a particular state\n",
    "all_possible_actions = ['U', 'D', 'R', 'L']\n",
    "\n",
    "small_enough = 1e-3\n",
    "\n",
    "alpha = 0.1\n",
    "\n",
    "# NOTE: if we use the standard grid, there's a good chance we will end up with\n",
    "# suboptimal policies\n",
    "# e.g.\n",
    "# ---------------------------\n",
    "#   R  |   R  |   R  |      |\n",
    "# ---------------------------\n",
    "#   R* |      |   U  |      |\n",
    "# ---------------------------\n",
    "#   U  |   R  |   U  |   L  |\n",
    "# since going R at (1,0) (shown with a *) incurs no cost, it's OK to keep doing that.\n",
    "# we'll either end up staying in the same spot, or back to the start (2,0), at which\n",
    "# point we whould then just go back up, or at (0,0), at which point we can continue\n",
    "# on right.\n",
    "# instead, let's penalize each movement so the agent will find a shorter route.\n",
    "#\n",
    "# grid = standard_grid()\n",
    "grid = negative_grid(step_cost=-0.1)\n",
    "\n",
    "# print rewards\n",
    "print(\"rewards:\")\n",
    "print_value(grid.rewards, grid)\n",
    "        \n",
    "# initialize Q(s, a), alpha(s, a)\n",
    "# create a dict Q with different states then embedded dictionary with different actions\n",
    "states = grid.all_states()\n",
    "Q = {}\n",
    "update_counts = {}\n",
    "sa_count = {}\n",
    "for s in states:\n",
    "    sa_count[s] = {}\n",
    "    Q[s] = {}\n",
    "    for a in all_possible_actions:\n",
    "        Q[s][a] = 0 # needs to be initialized to something so we can argmax it  \n",
    "        sa_count[s][a] = 1 # set initial count to be 0\n",
    "            \n",
    "# repeat until convergence\n",
    "# play game for n episodes\n",
    "for t in range(2000):\n",
    "    # set first state for starting position\n",
    "    s = (2, 0)\n",
    "    grid.set_state(s)\n",
    "    # choose an action based on max Q value\n",
    "    a = max_dict(Q[s])[0]\n",
    "    # loop until one episode is over\n",
    "    while not grid.game_over():\n",
    "        a = random_action(a)   # epsilon greedy\n",
    "        r = grid.move(a)\n",
    "        s1 = grid.current_state()\n",
    "        # we need the next action as well since Q(s,a) depends on Q(s',a')\n",
    "        # if s2 not in policy then it's a terminal state, all Q are 0\n",
    "        # the difference between SARSA and Q-Learning is with Q-Learning\n",
    "        # we will use this max[a']{ Q(s',a')} in our update\n",
    "        # even if we do not end up taking this action in the next step\n",
    "        a1, max_Q1 = max_dict(Q[s1])\n",
    "        # Q[s, a] = Q[s, a] + alpha*(r + gamma*max[a']{Q[s', a']} - Q[s, a])\n",
    "        # here we use alpha as adaptive learning rate like AdaGrad and RMSprop in DNN\n",
    "        # in this way when epsilon decreases for each episode it may miss the states which have never occur\n",
    "        # adaptive alpha will be high for such states and hence keeping the balance\n",
    "        sa_count[s][a] += 0.005\n",
    "        Q[s][a] = Q[s][a] + (alpha/sa_count[s][a])*(r + gamma*max_Q1 - Q[s][a])\n",
    "        \n",
    "        # we would like to know how often Q(s) has been updated too\n",
    "        update_counts[s] = update_counts.get(s,0) + 1\n",
    "        \n",
    "        # set current state as next state\n",
    "        s = s1\n",
    "        a = a1\n",
    "        \n",
    "# determine the policy from Q*\n",
    "# initialize policy, V\n",
    "policy, V = {}, {}\n",
    "for s in grid.actions.keys():\n",
    "    policy[s], V[s] = max_dict(Q[s])\n",
    "    \n",
    "print('policy:')\n",
    "print_policy(policy, grid)\n",
    "\n",
    "print(\"values:\")\n",
    "print_value(V, grid)\n",
    "\n",
    "# what's the proportion of time we spend updating each part of Q?\n",
    "print(\"update counts:\")\n",
    "total = np.sum(list(update_counts.values()))\n",
    "for k in update_counts.keys():\n",
    "    update_counts[k] =  float(update_counts[k]) / total\n",
    "print_value(update_counts, grid)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
