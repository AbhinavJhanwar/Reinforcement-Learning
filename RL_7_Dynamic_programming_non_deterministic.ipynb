{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## environment class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grid:\n",
    "    def __init__(self, rows, cols, start):\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.i = start[0]\n",
    "        self.j = start[1]\n",
    "\n",
    "        \n",
    "    def set(self, rewards, actions):\n",
    "        # rewards should be a dict of: (i, j): r (row, col): reward\n",
    "        # actions should be a dict of: (i, j): A (row, col): list of possible actions\n",
    "        self.rewards = rewards\n",
    "        self.actions = actions\n",
    "\n",
    "        \n",
    "    def set_state(self, s):\n",
    "        self.i = s[0]\n",
    "        self.j = s[1]\n",
    "\n",
    "        \n",
    "    def current_state(self):\n",
    "        return (self.i, self.j)\n",
    "\n",
    "    \n",
    "    def is_terminal(self, s):\n",
    "        return s not in self.actions\n",
    "\n",
    "    \n",
    "    def move(self, action):\n",
    "        # check if legal move first\n",
    "        if action in self.actions[(self.i, self.j)]:\n",
    "            if action == 'U':\n",
    "                self.i -= 1\n",
    "            elif action == 'D':\n",
    "                self.i += 1\n",
    "            elif action == 'R':\n",
    "                self.j += 1\n",
    "            elif action == 'L':\n",
    "                self.j -= 1\n",
    "        # return a reward (if any)\n",
    "        return self.rewards.get((self.i, self.j), 0)\n",
    "\n",
    "    \n",
    "    def undo_move(self, action):\n",
    "        # these are the opposite of what U/D/L/R should normally do\n",
    "        if action == 'U':\n",
    "            self.i += 1\n",
    "        elif action == 'D':\n",
    "            self.i -= 1\n",
    "        elif action == 'R':\n",
    "            self.j -= 1\n",
    "        elif action == 'L':\n",
    "            self.j += 1\n",
    "        # raise an exception if we arrive somewhere we shouldn't be\n",
    "        # should never happen\n",
    "        assert(self.current_state() in self.all_states())\n",
    "\n",
    "    \n",
    "    def game_over(self):\n",
    "        # returns true if game is over, else false\n",
    "        # true if we are in a state where no actions are possible\n",
    "        return (self.i, self.j) not in self.actions\n",
    "\n",
    "    \n",
    "    def all_states(self):\n",
    "        # possibly buggy but simple way to get all states\n",
    "        # either a position that has possible next actions\n",
    "        # or a position that yields a reward\n",
    "        return set(self.actions.keys()) | set(self.rewards.keys())\n",
    "\n",
    "\n",
    "def standard_grid():\n",
    "    # define a grid that describes the reward for arriving at each state\n",
    "    # and possible actions at each state\n",
    "    # the grid looks like this\n",
    "    # x means you can't go there\n",
    "    # s means start position\n",
    "    # number means reward at that state\n",
    "    # .  .  .  1\n",
    "    # .  x  . -1\n",
    "    # s  .  .  .\n",
    "    g = Grid(3, 4, (2, 0))\n",
    "    rewards = {(0, 3): 5, (1, 3): -5}\n",
    "    actions = {\n",
    "        (0, 0): ('D', 'R'),\n",
    "        (0, 1): ('L', 'R'),\n",
    "        (0, 2): ('L', 'D', 'R'),\n",
    "        (1, 0): ('U', 'D'),\n",
    "        (1, 2): ('U', 'D', 'R'),\n",
    "        (2, 0): ('U', 'R'),\n",
    "        (2, 1): ('L', 'R'),\n",
    "        (2, 2): ('L', 'R', 'U'),\n",
    "        (2, 3): ('L', 'U'),\n",
    "    }\n",
    "    g.set(rewards, actions)\n",
    "    return g\n",
    "\n",
    "\n",
    "def negative_grid(step_cost=-0.1):\n",
    "    # in this game we want to try to minimize the number of moves\n",
    "    # so we will penalize every move\n",
    "    g = standard_grid()\n",
    "    g.rewards.update({\n",
    "        (0, 0): step_cost,\n",
    "        (0, 1): step_cost,\n",
    "        (0, 2): step_cost,\n",
    "        (1, 0): step_cost,\n",
    "        (1, 2): step_cost,\n",
    "        (2, 0): step_cost,\n",
    "        (2, 1): step_cost,\n",
    "        (2, 2): step_cost,\n",
    "        (2, 3): step_cost,\n",
    "    })\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_value(V, g):\n",
    "    for i in range(g.rows):\n",
    "        print('----------------------------')\n",
    "        for j in range(g.cols):\n",
    "            v = V.get((i,j), 0)\n",
    "            if v>=0:\n",
    "                print('| %.2f '%v, end='')\n",
    "            else:\n",
    "                print('|%.2f '%v, end='') # negative sign takes up an extra space\n",
    "        print('|')\n",
    "    print('----------------------------')\n",
    "    \n",
    "\n",
    "def print_policy(P, g):\n",
    "    for i in range(g.rows):\n",
    "        print('-----------------')\n",
    "        for j in range(g.cols):\n",
    "            a = P.get((i,j), ' ')\n",
    "            print('| %s '%a, end='')\n",
    "        print('|')\n",
    "    print('-----------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## non deterministic approach or windy approach\n",
    "it means that what if environment doesn't allow agent to take the actions based on the given policy due to other external factors. This is called as windy problem <br>\n",
    "we will work on improving the policy as well here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial policy:\n",
      "-----------------\n",
      "| U | U | D |   |\n",
      "-----------------\n",
      "| D |   | R |   |\n",
      "-----------------\n",
      "| R | D | D | L |\n",
      "-----------------\n",
      "values for initial policy:\n",
      "----------------------------\n",
      "| 0.00 | 0.00 | 0.00 | 0.00 |\n",
      "----------------------------\n",
      "| 0.00 | 0.00 | 0.00 | 0.00 |\n",
      "----------------------------\n",
      "| 0.00 | 0.00 | 0.00 | 0.00 |\n",
      "----------------------------\n",
      "optimized policy:\n",
      "-----------------\n",
      "| R | R | R |   |\n",
      "-----------------\n",
      "| U |   | U |   |\n",
      "-----------------\n",
      "| U | R | U | L |\n",
      "-----------------\n",
      "values for optimized policy:\n",
      "----------------------------\n",
      "|-2.90 |-0.81 | 1.93 | 0.00 |\n",
      "----------------------------\n",
      "|-4.44 | 0.00 |-1.60 | 0.00 |\n",
      "----------------------------\n",
      "|-5.35 |-4.99 |-3.75 |-4.79 |\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "# threshold for convergence\n",
    "SMALL_ENOUGH = 10e-4\n",
    "\n",
    "# let's see how V(s) changes as we get further away from the reward\n",
    "gamma = 0.9 # discount factor\n",
    "\n",
    "# set different actions possible for a particular state\n",
    "all_possible_actions = ['U', 'D', 'R', 'L']\n",
    "\n",
    "# define grid in which all the steps will have a small negative reward\n",
    "# here if I increase the step_cost to -1 but keep the rewards to +1 and -1 only then when agent tries\n",
    "# to optimize the reward then it may end up in the loosing state as in that way overall rewards is\n",
    "# higher than what it will receive to reach to the winning state\n",
    "# hence terminal rewards should be such that it dominates all other state rewards\n",
    "# so increase the rewards from +/- 1 to +/-5 and will end up in winning state\n",
    "grid = negative_grid(step_cost=-1)\n",
    "\n",
    "# states will be positions (i,j)\n",
    "# simpler than tic-tac-toe because we only have one \"game piece\"\n",
    "# that can only be at one position at a time\n",
    "states = grid.all_states()\n",
    "\n",
    "# defining a random policy for each state\n",
    "policy = {}\n",
    "for s in grid.actions.keys():\n",
    "    policy[s] = np.random.choice(all_possible_actions)\n",
    "    \n",
    "print('initial policy:')\n",
    "print_policy(policy, grid)\n",
    "\n",
    "# initialize V(s) = 0\n",
    "V = {}\n",
    "for s in states:\n",
    "    V[s] = 0\n",
    "    \n",
    "print(\"values for initial policy:\")\n",
    "print_value(V, grid)\n",
    "\n",
    "# repeat until convergence - it will break when policy doesn't change\n",
    "# once policy doesn't change V value will also become constant\n",
    "while True:\n",
    "    # repeat until convergenece  for V\n",
    "    while True:\n",
    "        biggest_change = 0\n",
    "        for s in states:\n",
    "            old_v = V[s]\n",
    "            # V(s) only has value which is not 0 if it's not a terminal state\n",
    "            if s in policy:\n",
    "                # until now the action was deterministic so probability p=1  for the policy action\n",
    "                # and probability was 0 for other actions \n",
    "                # but now we are considering the probability of action in policy to be 0.5\n",
    "                # and rest of the actions have equal probability 0.5/3\n",
    "                new_v = 0\n",
    "                for a in all_possible_actions:\n",
    "                    if a == policy[s]:\n",
    "                        p = 0.5\n",
    "                    else:\n",
    "                        p = 0.5/3\n",
    "                    grid.set_state(s)\n",
    "                    r = grid.move(a)\n",
    "                    new_v += p*(r + gamma * V[grid.current_state()])\n",
    "                V[s] = new_v\n",
    "                biggest_change = max(biggest_change, np.abs(old_v-V[s]))\n",
    "\n",
    "        if biggest_change < SMALL_ENOUGH:\n",
    "            break\n",
    "            \n",
    "    # policy improvement step\n",
    "    is_policy_converged = True\n",
    "    for s in states:\n",
    "        if s in policy:\n",
    "            old_a = policy[s]\n",
    "            new_a = None\n",
    "            best_v = float('-inf')\n",
    "            # loop through all possible actions to get the best\n",
    "            for action in all_possible_actions:\n",
    "                v=0\n",
    "                for a2 in all_possible_actions:\n",
    "                    if a2==action:\n",
    "                        p=0.5\n",
    "                    else:\n",
    "                        p=0.5/3\n",
    "                    grid.set_state(s)\n",
    "                    r = grid.move(action)\n",
    "                    v += p*(r + gamma * V[grid.current_state()])\n",
    "                if v > best_v:\n",
    "                    best_v = v\n",
    "                    new_a = action\n",
    "            policy[s] = new_a\n",
    "            if new_a != old_a:\n",
    "                is_policy_converged = False\n",
    "    if is_policy_converged:\n",
    "        break\n",
    "\n",
    "print('optimized policy:')\n",
    "print_policy(policy, grid)\n",
    "\n",
    "print(\"values for optimized policy:\")\n",
    "print_value(V, grid)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
