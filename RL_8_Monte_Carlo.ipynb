{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## environment class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grid:\n",
    "    def __init__(self, rows, cols, start):\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.i = start[0]\n",
    "        self.j = start[1]\n",
    "\n",
    "        \n",
    "    def set(self, rewards, actions):\n",
    "        # rewards should be a dict of: (i, j): r (row, col): reward\n",
    "        # actions should be a dict of: (i, j): A (row, col): list of possible actions\n",
    "        self.rewards = rewards\n",
    "        self.actions = actions\n",
    "\n",
    "        \n",
    "    def set_state(self, s):\n",
    "        self.i = s[0]\n",
    "        self.j = s[1]\n",
    "\n",
    "        \n",
    "    def current_state(self):\n",
    "        return (self.i, self.j)\n",
    "\n",
    "    \n",
    "    def is_terminal(self, s):\n",
    "        return s not in self.actions\n",
    "\n",
    "    \n",
    "    def move(self, action):\n",
    "        # check if legal move first\n",
    "        if action in self.actions[(self.i, self.j)]:\n",
    "            if action == 'U':\n",
    "                self.i -= 1\n",
    "            elif action == 'D':\n",
    "                self.i += 1\n",
    "            elif action == 'R':\n",
    "                self.j += 1\n",
    "            elif action == 'L':\n",
    "                self.j -= 1\n",
    "        # return a reward (if any)\n",
    "        return self.rewards.get((self.i, self.j), 0)\n",
    "\n",
    "    \n",
    "    def undo_move(self, action):\n",
    "        # these are the opposite of what U/D/L/R should normally do\n",
    "        if action == 'U':\n",
    "            self.i += 1\n",
    "        elif action == 'D':\n",
    "            self.i -= 1\n",
    "        elif action == 'R':\n",
    "            self.j -= 1\n",
    "        elif action == 'L':\n",
    "            self.j += 1\n",
    "        # raise an exception if we arrive somewhere we shouldn't be\n",
    "        # should never happen\n",
    "        assert(self.current_state() in self.all_states())\n",
    "\n",
    "    \n",
    "    def game_over(self):\n",
    "        # returns true if game is over, else false\n",
    "        # true if we are in a state where no actions are possible\n",
    "        return (self.i, self.j) not in self.actions\n",
    "\n",
    "    \n",
    "    def all_states(self):\n",
    "        # possibly buggy but simple way to get all states\n",
    "        # either a position that has possible next actions\n",
    "        # or a position that yields a reward\n",
    "        return set(self.actions.keys()) | set(self.rewards.keys())\n",
    "\n",
    "\n",
    "def standard_grid():\n",
    "    # define a grid that describes the reward for arriving at each state\n",
    "    # and possible actions at each state\n",
    "    # the grid looks like this\n",
    "    # x means you can't go there\n",
    "    # s means start position\n",
    "    # number means reward at that state\n",
    "    # .  .  .  1\n",
    "    # .  x  . -1\n",
    "    # s  .  .  .\n",
    "    g = Grid(3, 4, (2, 0))\n",
    "    rewards = {(0, 3): 1, (1, 3): -1}\n",
    "    actions = {\n",
    "        (0, 0): ('D', 'R'),\n",
    "        (0, 1): ('L', 'R'),\n",
    "        (0, 2): ('L', 'D', 'R'),\n",
    "        (1, 0): ('U', 'D'),\n",
    "        (1, 2): ('U', 'D', 'R'),\n",
    "        (2, 0): ('U', 'R'),\n",
    "        (2, 1): ('L', 'R'),\n",
    "        (2, 2): ('L', 'R', 'U'),\n",
    "        (2, 3): ('L', 'U'),\n",
    "    }\n",
    "    g.set(rewards, actions)\n",
    "    return g\n",
    "\n",
    "\n",
    "def negative_grid(step_cost=-0.1):\n",
    "    # in this game we want to try to minimize the number of moves\n",
    "    # so we will penalize every move\n",
    "    g = standard_grid()\n",
    "    g.rewards.update({\n",
    "        (0, 0): step_cost,\n",
    "        (0, 1): step_cost,\n",
    "        (0, 2): step_cost,\n",
    "        (1, 0): step_cost,\n",
    "        (1, 2): step_cost,\n",
    "        (2, 0): step_cost,\n",
    "        (2, 1): step_cost,\n",
    "        (2, 2): step_cost,\n",
    "        (2, 3): step_cost,\n",
    "    })\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_value(V, g):\n",
    "    for i in range(g.rows):\n",
    "        print('----------------------------')\n",
    "        for j in range(g.cols):\n",
    "            v = V.get((i,j), 0)\n",
    "            if v>=0:\n",
    "                print('| %.2f '%v, end='')\n",
    "            else:\n",
    "                print('|%.2f '%v, end='') # negative sign takes up an extra space\n",
    "        print('|')\n",
    "    print('----------------------------')\n",
    "    \n",
    "\n",
    "def print_policy(P, g):\n",
    "    for i in range(g.rows):\n",
    "        print('-----------------')\n",
    "        for j in range(g.cols):\n",
    "            a = P.get((i,j), ' ')\n",
    "            print('| %s '%a, end='')\n",
    "        print('|')\n",
    "    print('-----------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo (MC) method\n",
    "advantage of this method is that we do not require to go through each and every state of game but the states we experience while playing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fixed policy - deterministic action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# let's see how V(s) changes as we get further away from the reward\n",
    "gamma = 0.9 # discount factor\n",
    "\n",
    "def play_game(grid, policy):\n",
    "    # returns a list of states and corresponding returns\n",
    "    # reset game to start at a random position\n",
    "    # we need to do this, because given our current deterministic policy\n",
    "    # we would never end up at certain states, but we still want to measure their value\n",
    "    \n",
    "    start_states = list(grid.actions.keys())\n",
    "    start_idx = np.random.choice(len(start_states))\n",
    "    grid.set_state(start_states[start_idx])\n",
    "\n",
    "    s = grid.current_state()\n",
    "    states_rewards = [(s, 0)]  # list of tuples (state, reward)\n",
    "    while not grid.game_over():\n",
    "        a = policy[s]\n",
    "        r = grid.move(a)\n",
    "        s = grid.current_state()\n",
    "        states_rewards.append((s, r))\n",
    "    # calculate the returns by working backwards from the terminal state\n",
    "    G=0\n",
    "    states_returns = []\n",
    "    first=True\n",
    "    for s, r in reversed(states_rewards):\n",
    "        # the value of the terminal state is 0 by definition\n",
    "        # we should ignore the first state we encounter\n",
    "        # and ignore the last G, which is meaningless since it doesn't correspond to any move\n",
    "        if first:\n",
    "            first = False\n",
    "        else:\n",
    "            states_returns.append((s, G))\n",
    "        G = r + gamma*G\n",
    "    states_returns.reverse() # we want it to be in order of state visited\n",
    "    return states_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "----------------------------\n",
      "| 0.00 | 0.00 | 0.00 | 1.00 |\n",
      "----------------------------\n",
      "| 0.00 | 0.00 | 0.00 |-1.00 |\n",
      "----------------------------\n",
      "| 0.00 | 0.00 | 0.00 | 0.00 |\n",
      "----------------------------\n",
      "policy:\n",
      "-----------------\n",
      "| R | R | R |   |\n",
      "-----------------\n",
      "| U |   | R |   |\n",
      "-----------------\n",
      "| U | R | R | U |\n",
      "-----------------\n",
      "values:\n",
      "----------------------------\n",
      "| 0.81 | 0.90 | 1.00 | 0.00 |\n",
      "----------------------------\n",
      "| 0.73 | 0.00 |-1.00 | 0.00 |\n",
      "----------------------------\n",
      "| 0.66 |-0.81 |-0.90 |-1.00 |\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "# use the standard grid again (0 for every step) so that we can compare\n",
    "# to iterative policy evaluation\n",
    "grid = standard_grid()\n",
    "\n",
    "# print rewards\n",
    "print(\"rewards:\")\n",
    "print_value(grid.rewards, grid)\n",
    "\n",
    "states = grid.all_states()\n",
    "\n",
    "# state -> action\n",
    "policy = {\n",
    "    (2, 0): 'U',\n",
    "    (1, 0): 'U',\n",
    "    (0, 0): 'R',\n",
    "    (0, 1): 'R',\n",
    "    (0, 2): 'R',\n",
    "    (1, 2): 'R',\n",
    "    (2, 1): 'R',\n",
    "    (2, 2): 'R',\n",
    "    (2, 3): 'U'\n",
    "}\n",
    "\n",
    "# initialize V(s) = 0, and returns where G values from different episodes will be saved for different states\n",
    "V = {}\n",
    "returns = {}\n",
    "for s in states:\n",
    "    V[s] = 0\n",
    "    returns[s] = []    \n",
    "    \n",
    "# play game for n episodes\n",
    "for t in range(100):\n",
    "    # generate an episode using pi\n",
    "    states_returns = play_game(grid, policy)\n",
    "    seen_states = set()\n",
    "    for s, G in states_returns:\n",
    "        # check if we have already seen s\n",
    "        # called \"first-visit\" MC policy evaluation\n",
    "        # in this if a state repeats again in a particular episode then we consider the value of only first visit\n",
    "        if s not in seen_states:\n",
    "            returns[s].append(G)\n",
    "            V[s] = np.mean(returns[s])\n",
    "            seen_states.add(s)\n",
    "            \n",
    "print('policy:')\n",
    "print_policy(policy, grid)\n",
    "\n",
    "print(\"values:\")\n",
    "print_value(V, grid)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when i play the game for say 100 times then it covers almost all the states and we get the same reward table as we were getting for markov decision process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## windy environment or non deterministic action\n",
    "notice that we are not optimizing policy here just general a new set of values which will be a little more negative compared to previous approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold for convergence\n",
    "SMALL_ENOUGH = 10e-4\n",
    "\n",
    "# let's see how V(s) changes as we get further away from the reward\n",
    "gamma = 0.9 # discount factor\n",
    "\n",
    "# set different actions possible for a particular state\n",
    "all_possible_actions = ['U', 'D', 'R', 'L']\n",
    "\n",
    "def random_action(a):\n",
    "    p = np.random.random()\n",
    "    if p<0.5:\n",
    "        return a\n",
    "    else:\n",
    "        return np.random.choice([action for action in all_possible_actions if action!=a])\n",
    "\n",
    "def play_game(grid, policy):\n",
    "    # returns a list of states and corresponding returns\n",
    "    # reset game to start at a random position\n",
    "    # we need to do this, because given our current deterministic policy\n",
    "    # we would never end up at certain states, but we still want to measure their value\n",
    "    \n",
    "    start_states = list(grid.actions.keys())\n",
    "    start_idx = np.random.choice(len(start_states))\n",
    "    grid.set_state(start_states[start_idx])\n",
    "\n",
    "    s = grid.current_state()\n",
    "    states_rewards = [(s, 0)]  # list of tuples (state, reward)\n",
    "    while not grid.game_over():\n",
    "        a = policy[s]\n",
    "        a = random_action(a)\n",
    "        r = grid.move(a)\n",
    "        s = grid.current_state() \n",
    "        states_rewards.append((s, r))\n",
    "    # calculate the returns by working backwards from the terminal state\n",
    "    G=0\n",
    "    states_returns = []\n",
    "    first=True\n",
    "    for s, r in reversed(states_rewards):\n",
    "        # the value of the terminal state is 0 by definition\n",
    "        # we should ignore the first state we encounter\n",
    "        # and ignore the last G, which is meaningless since it doesn't correspond to any move\n",
    "        if first:\n",
    "            first = False\n",
    "        else:\n",
    "            states_returns.append((s, G))\n",
    "        G = r + gamma*G\n",
    "    states_returns.reverse() # we want it to be in order of state visited\n",
    "    return states_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "----------------------------\n",
      "| 0.00 | 0.00 | 0.00 | 1.00 |\n",
      "----------------------------\n",
      "| 0.00 | 0.00 | 0.00 |-1.00 |\n",
      "----------------------------\n",
      "| 0.00 | 0.00 | 0.00 | 0.00 |\n",
      "----------------------------\n",
      "policy:\n",
      "-----------------\n",
      "| R | R | R |   |\n",
      "-----------------\n",
      "| U |   | U |   |\n",
      "-----------------\n",
      "| U | L | U | L |\n",
      "-----------------\n",
      "values:\n",
      "----------------------------\n",
      "| 0.43 | 0.55 | 0.73 | 0.00 |\n",
      "----------------------------\n",
      "| 0.33 | 0.00 | 0.23 | 0.00 |\n",
      "----------------------------\n",
      "| 0.25 | 0.18 | 0.12 |-0.15 |\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "# use the standard grid again (0 for every step) so that we can compare\n",
    "# to iterative policy evaluation\n",
    "grid = standard_grid()\n",
    "\n",
    "# set different actions possible for a particular state\n",
    "all_possible_actions = ['U', 'D', 'R', 'L']\n",
    "\n",
    "# print rewards\n",
    "print(\"rewards:\")\n",
    "print_value(grid.rewards, grid)\n",
    "\n",
    "states = grid.all_states()\n",
    "\n",
    "# defining a initial policy\n",
    "policy = {\n",
    "    (2, 0): 'U',\n",
    "    (1, 0): 'U',\n",
    "    (0, 0): 'R',\n",
    "    (0, 1): 'R',\n",
    "    (0, 2): 'R',\n",
    "    (1, 2): 'U',\n",
    "    (2, 1): 'L',\n",
    "    (2, 2): 'U',\n",
    "    (2, 3): 'L',\n",
    "  }\n",
    "\n",
    "\n",
    "# initialize V(s) = 0, and returns where G values from different episodes will be saved for different states\n",
    "V = {}\n",
    "returns = {}\n",
    "for s in states:\n",
    "    V[s] = 0\n",
    "    returns[s] = []    \n",
    "    \n",
    "# play game for n episodes\n",
    "for t in range(5000):\n",
    "    # generate an episode using pi\n",
    "    states_returns = play_game(grid, policy)\n",
    "    seen_states = set()\n",
    "    for s, G in states_returns:\n",
    "        # check if we have already seen s\n",
    "        # called \"first-visit\" MC policy evaluation\n",
    "        # in this if a state repeats again in a particular episode then we consider the value of only first visit\n",
    "        if s not in seen_states:\n",
    "            returns[s].append(G)\n",
    "            V[s] = np.mean(returns[s])\n",
    "            seen_states.add(s)\n",
    "            \n",
    "print('policy:')\n",
    "print_policy(policy, grid)\n",
    "\n",
    "print(\"values:\")\n",
    "print_value(V, grid)\n",
    "\n",
    "# state -> action\n",
    "# found by policy_iteration_random on standard_grid\n",
    "# MC method won't get exactly this, but should be close\n",
    "# values:\n",
    "# ---------------------------\n",
    "#  0.43|  0.56|  0.72|  0.00|\n",
    "# ---------------------------\n",
    "#  0.33|  0.00|  0.21|  0.00|\n",
    "# ---------------------------\n",
    "#  0.25|  0.18|  0.11| -0.17|\n",
    "# policy:\n",
    "# ---------------------------\n",
    "#   R  |   R  |   R  |      |\n",
    "# ---------------------------\n",
    "#   U  |      |   U  |      |\n",
    "# ---------------------------\n",
    "#   U  |   L  |   U  |   L  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mc control method i.e. optimizing policy\n",
    "here to optimize policy we use Q values instead of V values bcos Q values consider not just state but action as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(grid, policy):\n",
    "    # returns a list of states and corresponding returns\n",
    "    # reset game to start at a random position\n",
    "    # we need to do this, because given our current deterministic policy\n",
    "    # we would never end up at certain states, but we still want to measure their value\n",
    "    # this is called the \"exploring starts\" method\n",
    "    \n",
    "    start_states = list(grid.actions.keys())\n",
    "    start_idx = np.random.choice(len(start_states))\n",
    "    grid.set_state(start_states[start_idx])\n",
    "\n",
    "    s = grid.current_state()\n",
    "    a = np.random.choice(all_possible_actions)  # first action is uniformly random\n",
    "    \n",
    "    # be aware of the timing\n",
    "    # each triple is s(t), a(t), r(t)\n",
    "    # but r(t) results from taking action a(t-1) from s(t-1) and landing in s(t)\n",
    "    states_actions_rewards = [(s, a, 0)]  # list of tuples (state, action, reward)\n",
    "    \n",
    "    # hack so that we do not end up in a very long episode when we have a random policy\n",
    "    seen_states = set()\n",
    "    seen_states.add(grid.current_state())\n",
    "    while True:\n",
    "        old_s = grid.current_state()\n",
    "        r = grid.move(a)\n",
    "        s = grid.current_state()\n",
    "        if s in seen_states:\n",
    "            states_actions_rewards.append((s, None, -10))\n",
    "            break\n",
    "        elif grid.game_over():\n",
    "            states_actions_rewards.append((s, None, r))\n",
    "            break\n",
    "        else:\n",
    "            a = policy[s]\n",
    "            states_actions_rewards.append((s, a, r))\n",
    "        seen_states.add(s)\n",
    "        \n",
    "    # calculate the returns by working backwards from the terminal state\n",
    "    G=0\n",
    "    state_action_returns = []\n",
    "    first=True\n",
    "    for s, a, r in reversed(states_actions_rewards):\n",
    "        # the value of the terminal state is 0 by definition\n",
    "        # we should ignore the first state we encounter\n",
    "        # and ignore the last G, which is meaningless since it doesn't correspond to any move\n",
    "        if first:\n",
    "            first = False\n",
    "        else:\n",
    "            state_action_returns.append((s, a, G))\n",
    "        G = r + gamma*G\n",
    "    state_action_returns.reverse() # we want it to be in order of state visited\n",
    "    return state_action_returns\n",
    "\n",
    "def max_dict(d):\n",
    "    # returns the argmax (key) and max (value) from a dictionary\n",
    "    max_key = None\n",
    "    max_value = float('-inf')\n",
    "    for key, value in d.items():\n",
    "        if value>max_value:\n",
    "            max_key = key\n",
    "            max_value = value\n",
    "    return max_key, max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "----------------------------\n",
      "| 0.00 | 0.00 | 0.00 | 10.00 |\n",
      "----------------------------\n",
      "| 0.00 | 0.00 | 0.00 |-10.00 |\n",
      "----------------------------\n",
      "| 0.00 | 0.00 | 0.00 | 0.00 |\n",
      "----------------------------\n",
      "policy:\n",
      "-----------------\n",
      "| R | R | R |   |\n",
      "-----------------\n",
      "| U |   | U |   |\n",
      "-----------------\n",
      "| R | R | U | L |\n",
      "-----------------\n",
      "final values:\n",
      "----------------------------\n",
      "| 2.50 | 4.72 | 10.00 | 0.00 |\n",
      "----------------------------\n",
      "| 2.37 | 0.00 | 6.15 | 0.00 |\n",
      "----------------------------\n",
      "| 2.25 | 3.53 | 5.56 |-1.52 |\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "# let's see how V(s) changes as we get further away from the reward\n",
    "gamma = 0.9 # discount factor\n",
    "\n",
    "# set different actions possible for a particular state\n",
    "all_possible_actions = ['U', 'D', 'R', 'L']\n",
    "\n",
    "# use the standard grid again (0 for every step) so that we can compare\n",
    "# to iterative policy evaluation\n",
    "grid = standard_grid()\n",
    "\n",
    "states = grid.all_states()\n",
    "\n",
    "# print rewards\n",
    "print(\"rewards:\")\n",
    "print_value(grid.rewards, grid)\n",
    "\n",
    "# random initialize policy\n",
    "policy = {}\n",
    "for s in grid.actions.keys():\n",
    "    policy[s] = np.random.choice(all_possible_actions)\n",
    "    \n",
    "# initialize Q(s, a) & returns\n",
    "# create a dict Q with different states then embedded dictionary with different actions\n",
    "Q = {}\n",
    "returns = {} # dictionary of state -> list of returns we've received\n",
    "for s in states:\n",
    "    if s in grid.actions: # not a terminal state\n",
    "        Q[s] = {}\n",
    "        for a in all_possible_actions:\n",
    "            Q[s][a] = 0 # needs to be initialized to something so we can argmax it\n",
    "            returns[(s, a)] = []\n",
    "\n",
    "# repeat until convergence\n",
    "# play game for n episodes\n",
    "for t in range(2000):\n",
    "    # generate an episode using pi\n",
    "    states_actions_returns = play_game(grid, policy)\n",
    "    seen_states_actions = set()\n",
    "    for s, a, G in states_actions_returns:\n",
    "        # check if we have already seen s\n",
    "        # called \"first-visit\" MC policy evaluation\n",
    "        # in this if a state repeats again in a particular episode then we consider the value of only first visit\n",
    "        sa = (s, a)\n",
    "        if sa not in seen_states_actions:\n",
    "            returns[sa].append(G)\n",
    "            Q[s][a] = np.mean(returns[sa])\n",
    "            seen_states_actions.add(sa)\n",
    "\n",
    "    # update policy\n",
    "    for s in policy.keys():\n",
    "        policy[s] = max_dict(Q[s])[0]\n",
    "\n",
    "print('policy:')\n",
    "print_policy(policy, grid)\n",
    "\n",
    "# find V\n",
    "V = {}\n",
    "for s, Qs in Q.items():\n",
    "    V[s] = max_dict(Q[s])[1]\n",
    "\n",
    "print(\"final values:\")\n",
    "print_value(V, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mc control method with windy environment i.e. optimizing policy without exploring start position and random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_action(a, eps=0.1):\n",
    "    # choose given a with probability 1 - eps + eps/4\n",
    "    # choose some other a' != a with probability eps/4\n",
    "    p = np.random.random()\n",
    "    \n",
    "    # if p < (1 - eps + eps/len(ALL_POSSIBLE_ACTIONS)):\n",
    "    #   return a\n",
    "    # else:\n",
    "    #   tmp = list(ALL_POSSIBLE_ACTIONS)\n",
    "    #   tmp.remove(a)\n",
    "    #   return np.random.choice(tmp)\n",
    "    #\n",
    "    # this is equivalent to the above\n",
    "    if p<(1-eps):\n",
    "        return a\n",
    "    else:\n",
    "        return np.random.choice(all_possible_actions)\n",
    "\n",
    "def play_game(grid, policy):\n",
    "    # returns a list of states and corresponding returns\n",
    "    # in this version we will NOT use \"exploring starts\" method\n",
    "    # instead we will explore using an epsilon-soft policy\n",
    "\n",
    "    s = (2, 0)\n",
    "    grid.set_state(s)\n",
    "\n",
    "    a = random_action(policy[s])\n",
    "    \n",
    "    # be aware of the timing\n",
    "    # each triple is s(t), a(t), r(t)\n",
    "    # but r(t) results from taking action a(t-1) from s(t-1) and landing in s(t)\n",
    "    states_actions_rewards = [(s, a, 0)]  # list of tuples (state, action, reward)\n",
    "    \n",
    "    while True:\n",
    "        r = grid.move(a)\n",
    "        s = grid.current_state()\n",
    "        if grid.game_over():\n",
    "            states_actions_rewards.append((s, None, r))\n",
    "            break\n",
    "        else:\n",
    "            a = random_action(policy[s]) # the next state is stochastic\n",
    "            states_actions_rewards.append((s, a, r))\n",
    "        \n",
    "    # calculate the returns by working backwards from the terminal state\n",
    "    G=0\n",
    "    state_action_returns = []\n",
    "    first=True\n",
    "    for s, a, r in reversed(states_actions_rewards):\n",
    "        # the value of the terminal state is 0 by definition\n",
    "        # we should ignore the first state we encounter\n",
    "        # and ignore the last G, which is meaningless since it doesn't correspond to any move\n",
    "        if first:\n",
    "            first = False\n",
    "        else:\n",
    "            state_action_returns.append((s, a, G))\n",
    "        G = r + gamma*G\n",
    "    state_action_returns.reverse() # we want it to be in order of state visited\n",
    "    return state_action_returns\n",
    "\n",
    "def max_dict(d):\n",
    "    # returns the argmax (key) and max (value) from a dictionary\n",
    "    max_key = None\n",
    "    max_value = float('-inf')\n",
    "    for key, value in d.items():\n",
    "        if value>max_value:\n",
    "            max_key = key\n",
    "            max_value = value\n",
    "    return max_key, max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "----------------------------\n",
      "|-0.10 |-0.10 |-0.10 | 1.00 |\n",
      "----------------------------\n",
      "|-0.10 | 0.00 |-0.10 |-1.00 |\n",
      "----------------------------\n",
      "|-0.10 |-0.10 |-0.10 |-0.10 |\n",
      "----------------------------\n",
      "policy:\n",
      "-----------------\n",
      "| R | R | R |   |\n",
      "-----------------\n",
      "| U |   | U |   |\n",
      "-----------------\n",
      "| U | R | U | L |\n",
      "-----------------\n",
      "final values:\n",
      "----------------------------\n",
      "| 0.58 | 0.77 | 1.00 | 0.00 |\n",
      "----------------------------\n",
      "| 0.41 | 0.00 | 0.78 | 0.00 |\n",
      "----------------------------\n",
      "| 0.25 | 0.33 | 0.53 | 0.46 |\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "# let's see how V(s) changes as we get further away from the reward\n",
    "gamma = 0.9 # discount factor\n",
    "\n",
    "# set different actions possible for a particular state\n",
    "all_possible_actions = ['U', 'D', 'R', 'L']\n",
    "\n",
    "# use the standard grid again (0 for every step) so that we can compare\n",
    "# to iterative policy evaluation\n",
    "grid = negative_grid(step_cost=-0.1)\n",
    "\n",
    "states = grid.all_states()\n",
    "\n",
    "# print rewards\n",
    "print(\"rewards:\")\n",
    "print_value(grid.rewards, grid)\n",
    "\n",
    "# random initialize policy\n",
    "policy = {}\n",
    "for s in grid.actions.keys():\n",
    "    policy[s] = np.random.choice(all_possible_actions)\n",
    "    \n",
    "# initialize Q(s, a) & returns\n",
    "# create a dict Q with different states then embedded dictionary with different actions\n",
    "Q = {}\n",
    "returns = {} # dictionary of state -> list of returns we've received\n",
    "for s in states:\n",
    "    if s in grid.actions: # not a terminal state\n",
    "        Q[s] = {}\n",
    "        for a in all_possible_actions:\n",
    "            Q[s][a] = 0 # needs to be initialized to something so we can argmax it\n",
    "            returns[(s, a)] = []\n",
    "\n",
    "# repeat until convergence\n",
    "# play game for n episodes\n",
    "for t in range(5000):\n",
    "    # generate an episode using pi\n",
    "    states_actions_returns = play_game(grid, policy)\n",
    "    seen_states_actions = set()\n",
    "    for s, a, G in states_actions_returns:\n",
    "        # check if we have already seen s\n",
    "        # called \"first-visit\" MC policy evaluation\n",
    "        # in this if a state repeats again in a particular episode then we consider the value of only first visit\n",
    "        sa = (s, a)\n",
    "        if sa not in seen_states_actions:\n",
    "            returns[sa].append(G)\n",
    "            Q[s][a] = np.mean(returns[sa])\n",
    "            seen_states_actions.add(sa)\n",
    "\n",
    "    # calculate new policy pi(s) = argmax[a]{ Q(s,a) }\n",
    "    for s in policy.keys():\n",
    "        policy[s] = max_dict(Q[s])[0]\n",
    "\n",
    "print('policy:')\n",
    "print_policy(policy, grid)\n",
    "\n",
    "# find V\n",
    "V = {}\n",
    "for s in policy.keys():\n",
    "    V[s] = max_dict(Q[s])[1]\n",
    "\n",
    "print(\"final values:\")\n",
    "print_value(V, grid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
